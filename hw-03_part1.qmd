---
title: "hw-03_part1"
subtitle: "Classification: Basic Concepts and Techniques"
author: "Kristi Manasil"
gitid: "kmanasil"
format: html
editor: visual
---

# **Classification: Basic Concepts and Techniques**

## **Install packages**

Install the packages used in this chapter:

```{r}
if(!require(pacman))
  install.packages("pacman")

pacman::p_load(tidyverse, rpart, rpart.plot, caret, 
  lattice, FSelector, sampling, pROC, mlbench)
```

## **Introduction**

Classification is a machine learning task with the goal to learn a predictive function of the form

y=f(x),

where x is called the attribute set and y the class label. The attribute set consists of feature which describe an object. These features can be measured using any scale (i.e., nominal, interval, ...). The class label is a nominal attribute. It it is a binary attribute, then the problem is called a binary classification problem.

Classification learns the classification model from training data where both the features and the correct class label are available. This is why it is called a [supervised learning problem](https://en.wikipedia.org/wiki/Supervised_learning).

A related supervised learning problem is [regression](https://en.wikipedia.org/wiki/Linear_regression), where y is a number instead of a label. Linear regression is a very popular supervised learning model, however, we will not talk about it here since it is taught in almost any introductory statistics course.

This chapter will introduce decision trees, model evaluation and comparison, feature selection, and then explore methods to handle the class imbalance problem.

You can read the free sample chapter from the textbook \[\@Tan2005\]: [Chapter 3. Classification: Basic Concepts and Techniques](https://www-users.cs.umn.edu/~kumar001/dmbook/ch3_classification.pdf)

## **The Bird FeederWatch Dataset**

To demonstrate classification, I will use the two bird dataset which I will merge into a single frame.

```{r}
library(tidyverse)
# read in first data frame
bird1 <- read.csv("data/PFW_2021_public.csv")
head(bird1)
```

```{r}
# rename PROJ_PERIOD_ID to lower case to match second df
bird1 <- bird1 %>% rename(proj_period_id = PROJ_PERIOD_ID)
```

```{r}
# read in second data frame
bird2 <- read.csv("data/PFW_count_site_data_public_2021.csv")
head(bird2)
```

```{r}
```

```{r}
# keep only the PFW_2021 obserevations from bird2.
bird2 <- bird2 %>% filter(proj_period_id == "PFW_2021")
head(bird2)
```

I am going to merge these into one data frame by using the loc_id and proj_period_id columns. And remove na values.

```{r}
# merge into 1 frame.
bird_df <- merge(bird1, bird2, by=c("loc_id", "proj_period_id"))
bird_df %>% glimpse()
```

```{r}
# drop columns that appear to be all na
bird_df <- select(bird_df, -c("numfeeders_hanging", "numfeeders_water", "numfeeders_thistle"))
```

```{r}
# remove other rows with na values
bird_df <- na.omit(bird_df)
```

```{r}
# display as tibble
library(tidyverse)
as_tibble(bird_df)
```

This still has 79 columns, so I am going to cut out a few that I am not going to use

```{r}
# make the obs_id the index
rownames(bird_df) <- bird_df$obs_id
# remove columns
bird_df <- select(bird_df, -c(loc_id, proj_period_id, Data_Entry_Method, latitude, longitude, subnational1_code, entry_technique, sub_id, Month, Day, Year, how_many, valid, reviewed, day1_am, day1_pm, day2_am, day2_pm, effort_hrs_atleast, snow_dep_atleast, population_atleast, count_area_size_sq_m_atleast))
```

```{r}
# remove the obs_id column
bird_df <- select(bird_df, -c(obs_id))
head(bird_df)
```

This is still a lot of variable to work with so I am going to remove the atleast columns and the fed_in columns.

```{r}
# remove some more columns
bird_df<-select(bird_df, -c(21:29))

```

```{r}
# and remove some more
bird_df<-select(bird_df, -c(28:47))
```

```{r}
# 61113 observations is a little much so I am going to try using just the first 1000 rows
bird_df <- bird_df[1:1000,]
# Okay to make this more usable, lets just consider 4 species
bird_df <- bird_df %>% filter(species_code == "daejun"|species_code == "amegfi"|species_code == "bkcchi"| species_code == "blujay"|species_code == "tuftit")
# lets also rename the species code to just species
bird_df <- bird_df %>% rename(species = species_code)
```

```{r}
# make 1 and 0 logical values to mimic example problem
bird_df <- bird_df %>% mutate_at(c('yard_type_pavement','yard_type_garden', 'yard_type_landsca', 'yard_type_woods', "yard_type_desert", 'hab_dcid_woods','hab_evgr_woods', 'hab_mixed_woods', 'hab_orchard', 'hab_park','hab_water_fresh','hab_water_salt',  'hab_residential', 'hab_industrial', 'hab_agricultural', 'hab_desert_scrub', 'hab_young_woods', 'hab_swamp', 'hab_marsh', 'nearby_feeders', "squirrels", 'cats', 'dogs', 'humans', 'fed_yr_round' ),as.logical)

```

```{r}

bird_df <- bird_df %>% 
  mutate(across(where(is.logical), factor, levels = c(TRUE, FALSE))) %>% 
  mutate(across(where(is.character), factor))
head(bird_df)
```

```{r}
summary(bird_df)
```

```{r}
# remove a few more
bird_df<-select(bird_df, -c(2:3,6,8,10,13:14,17,22, 27))

summary(bird_df)
```

Okay I think I have a dataset that will work!

## **Decision Trees**

Recursive Partitioning (similar to CART) uses the Gini index to make splitting decisions and early stopping (pre-pruning).

```{r}
library(rpart)
```

### **Create Tree With Default Settings (uses pre-pruning)**

```{r}
tree_default <- bird_df |>
  rpart(species~.,data=_)
tree_default
```

This is not as easy to understand as the example but lets see how it looks

```{r}
library(rpart.plot)
rpart.plot(tree_default, extra = 2)
```

### **Create a Full Tree**

To create a full tree, we set the complexity parameter cp to 0 (split even if it does not improve the tree) and we set the minimum number of observations in a node needed to split to the smallest value of 2 (see: `?rpart.control`). *Note:* full trees overfit the training data!

```{r}
tree_full <- rpart(species ~.,data=bird_df,
                   control = rpart.control(minsplit = 2, cp = 0))
rpart.plot(tree_full, extra = 2, 
           roundint=FALSE,
            box.palette = list("Gy", "Gn", "Bu", "Bn","Or", "Rd", "Pu")) # specify 4 colors
```

Clear as mud, Right?

```{r}
tree_full
```

Training error on tree with pre-pruning

```{r}
predict(tree_default, bird_df) |> head ()
```

Does not appear to be doing a good good predicting

```{r}
pred <- predict(tree_default, bird_df, type="class")
head(pred)
```

```{r}
confusion_table <- with(bird_df, table(species, pred))
confusion_table
```

```{r}
correct <- confusion_table |> diag() |> sum()
correct
```

```{r}
error <- confusion_table |> sum() - correct
error
```

```{r}
accuracy <- correct / (correct + error)
accuracy
```

Not a very good result with an accuracy of just over 50%

```{r}
accuracy <- function(truth, prediction) {
    tbl <- table(truth, prediction)
    sum(diag(tbl))/sum(tbl)
}

accuracy(bird_df |> pull(species), pred)
```

Training error of the full tree

```{r}
accuracy(bird_df |> pull(species),
         predict(tree_full, bird_df, type="class"))
```

Still not great. Probably a good sign that the variables selected cannot be used to predict the bird species observed.

```{r}
library(caret)
confusionMatrix(data = pred, 
                reference = bird_df |> pull(species))
```

### **Make Predictions for New Data**

Make up my own animal: A lion with feathered wings

```{r}
my_bird <- tibble(species = NA, yard_type_landsca = TRUE, yard_type_woods = FALSE, hab_dcid_woods=FALSE,hab_mixed_woods=TRUE,hab_park=TRUE, hab_water_fresh=FALSE, hab_industrial=TRUE, hab_agricultural=FALSE, hab_young_woods=TRUE, hab_swamp=FALSE, hab_marsh=FALSE, nearby_feeders=TRUE, cats=FALSE, dogs=TRUE, humans=TRUE, housing_density=3)
```

Fix columns to be factors like in the training set.

```{r}
my_bird <- my_bird |> 
  mutate(across(where(is.logical), factor, levels = c(TRUE, FALSE)))
my_bird
```

Make a prediction using the default tree

```{r}
predict(tree_default , my_bird, type = "class")
```

## **Model Evaluation with Caret**

The package [`caret`](https://topepo.github.io/caret/) makes preparing training sets, building classification (and regression) models and evaluation easier. A great cheat sheet can be found [here](https://ugoproto.github.io/ugo_r_doc/pdf/caret.pdf).

```{r}
library(caret)
```

Set random number generator seed to make results reproducible

```{r}
set.seed(2000)
```

### **Hold out Test Data**

Test data is not used in the model building process and set aside purely for testing the model. Here, we partition data the 80% training and 20% testing.

```{r}
# create training data frame
inTrain <- createDataPartition(y = bird_df$species, p = .8, list = FALSE)
bird_train <- bird_df |> slice(inTrain)
```

```{r}
# create testing data set
bird_test <- bird_df |> slice(-inTrain)
```

### **Learn a Model and Tune Hyperparameters on the Training Data**

The package `caret` combines training and validation for hyperparameter tuning into a single function called `train()`. It internally splits the data into training and validation sets and thus will provide you with error estimates for different hyperparameter settings. `trainControl` is used to choose how testing is performed.

For rpart, train tries to tune the `cp` parameter (tree complexity) using accuracy to chose the best model. I set `minsplit` to 10 in order to get more interesting results as 2 was to low and no tree structure was produce. **Note:** Parameters used for tuning (in this case `cp`) need to be set using a data.frame in the argument `tuneGrid`! Setting it in control will be ignored.

```{r}
fit <- bird_train |>
  train(species ~ .,
    data = _ ,
    method = "rpart",
    control = rpart.control(minsplit = 10),
    trControl = trainControl(method = "cv", number = 10),
    tuneLength = 5)

fit
```

**Note:** Train has built 10 trees using the training folds for each value of `cp` and the reported values for accuracy and Kappa are the averages on the validation folds.

A model using the best tuning parameters and using all the data supplied to `train()` is available as `fit$finalModel`.

```{r}
rpart.plot(fit$finalModel, extra = 2,
  box.palette = list("Gy", "Gn", "Bu", "Bn", "Or", "Rd", "Pu"))
```

I am surprised by this result as I excepted to have more of tree structure despite the low accuracy and kappa.

caret also computes variable importance. By default it uses competing splits (splits which would be runners up, but do not get chosen by the tree) for rpart models (see `? varImp`). Toothed is the runner up for many splits, but it never gets chosen!

```{r}
varImp(fit)
```

Here is the variable importance without competing splits.

```{r}
imp <- varImp(fit, compete = FALSE)
imp
```

```{r}
ggplot(imp)
```

This may help explain the poor accuracy

## **Testing: Confusion Matrix and Confidence Interval for Accuracy**

Use the best model on the test data

```{r}
pred <- predict(fit, newdata = bird_test)
pred
```

Caret's `confusionMatrix()` function calculates accuracy, confidence intervals, kappa and many more evaluation metrics. You need to use separate test data to create a confusion matrix based on the generalization error.

```{r}
confusionMatrix(data = pred, 
                ref = bird_test |> pull(species))
```

This has low accuracy and even lower kappa score. The p-value is also not significant and e cannot reject the null hypothesis.

## **Model Comparison**

We will compare decision trees with a k-nearest neighbors (kNN) classifier. We will create fixed sampling scheme (10-folds) so we compare the different models using exactly the same folds. It is specified as `trControl` during training.

```{r}
train_index <- createFolds(bird_train$species, k = 10)
```

Build models

```{r}
rpartFit <- bird_train |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

**Note:** for kNN we ask `train` to scale the data using `preProcess = "scale"`. Logicals will be used as 0-1 variables in Euclidean distance calculation.

```{r}
knnFit <- bird_train |> 
  train(species ~ .,
        data = _,
        method = "knn",
        preProcess = "scale",
          tuneLength = 10,
          trControl = trainControl(method = "cv", indexOut = train_index)
  )
```

Compare accuracy over all folds.

```{r}
resamps <- resamples(list(
        CART = rpartFit,
        kNearestNeighbors = knnFit
        ))

summary(resamps)
```

`caret` provides some visualizations using the package `lattice`. For example, a boxplot to compare the accuracy and kappa distribution (over the 10 folds).

```{r}
library(lattice)
bwplot(resamps, layout = c(3, 1))
```

I would say in that both of these models preform rather poorly but if I had to pick one the kNearestNeighbors has small whiskers(less variance) and that is the one I would use.

Find out if one models is statistically better than the other (is the difference in accuracy is not zero).

```{r}
difs <- diff(resamps)
difs
```

```{r}
summary(difs)
```

p-values tells you the probability of seeing an even more extreme value (difference between accuracy) given that the null hypothesis (difference = 0) is true. For a better classifier, the p-value should be less than .05 or 0.01. `diff` automatically applies Bonferroni correction for multiple comparisons. In this case, kNN seems better but the classifiers do not perform statistically differently. Here we can see that neither model has a p-value less than .05 and a diff much closer to 0.

## **Feature Selection and Feature Preparation**

Decision trees implicitly select features for splitting, but we can also select features manually.

```{r}
library(FSelector)
```

### **Univariate Feature Importance Score**

These scores measure how related each feature is to the class variable. For discrete features (as in our case), the chi-square statistic can be used to derive a score.

```{r}
weights <- bird_train |> 
  chi.squared(species ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))

weights
```

plot importance in descending order (using `reorder` to order factor levels used by `ggplot`).

```{r}
ggplot(weights,
  aes(x = attr_importance, y = reorder(feature, attr_importance))) +
  geom_bar(stat = "identity") +
  xlab("Importance score") + 
  ylab("Feature")
```

The highest weight is less than 0.4. We will still get the best 5 features.

```{r}
subset <- cutoff.k(weights |> 
                   column_to_rownames("feature"), 5)
subset
```

Use only the best 5 features to build a model (`Fselector` provides `as.simple.formula`)

```{r}
f <- as.simple.formula(subset, "species")
f
```

```{r}
m <- bird_train |> rpart(f, data = _)
rpart.plot(m, extra = 2, roundint = FALSE)
```

There are many alternative ways to calculate univariate importance scores (see package FSelector). Some of them (also) work for continuous features. One example is the information gain ratio based on entropy as used in decision tree induction.

```{r}
bird_train |> 
  gain.ratio(species ~ ., data = _) |>
  as_tibble(rownames = "feature") |>
  arrange(desc(attr_importance))
```

I probably picked the wrong dataset for this HW as I am running out of ways to says that these attributes are not good for predicting the type of bird species.

### **Feature Subset Selection**

Often features are related and calculating importance for each feature independently is not optimal. We can use greedy search heuristics. For example `cfs` uses correlation/entropy with best first search.

```{r}
bird_train |> 
  cfs(species ~ ., data = _)
```

Black-box feature selection uses an evaluator function (the black box) to calculate a score to be maximized. First, we define an evaluation function that builds a model given a subset of features and calculates a quality score. We use here the average for 5 bootstrap samples (`method = "cv"` can also be used instead), no tuning (to be faster), and the average accuracy as the score.

```{r}
evaluator <- function(subset) {
  model <- bird_train |> 
    train(as.simple.formula(subset, "species"),
          data = _,
          method = "rpart",
          trControl = trainControl(method = "boot", number = 5),
          tuneLength = 0)
  results <- model$resample$Accuracy
  cat("Trying features:", paste(subset, collapse = " + "), "\n")
  m <- mean(results)
  cat("Accuracy:", round(m, 2), "\n\n")
  m
}
```

Start with all features (but not the class variable species)

```{r}
features <- bird_train |> colnames() |> setdiff("species")
```

There are several (greedy) search strategies available. These run for a while!

```{r}
subset_1 <- backward.search(features, evaluator)
subset_1
```

```{r}
subset_2 <- forward.search(features, evaluator)
subset_2
```

```{r}
subset_3 <- best.first.search(features, evaluator)
subset_3
```

```{r}
subset_4 <- hill.climbing.search(features, evaluator)
subset_4
```

It appears that hill climbing (subset_4) gave the best accuracy with four features at 29%.

### **Using Dummy Variables for Factors**

Nominal features (factors) are often encoded as a series of 0-1 dummy variables. For example, let us try to predict if an bird lives in hab_park given the species. First we use the original encoding of type as a factor with several values.

```{r}
tree_hab_park <- bird_train |> 
  rpart(hab_park ~ species, data = _)
rpart.plot(tree_hab_park, extra = 2, roundint = FALSE)
```

**Note:** Some splits use multiple values. Building the tree will become extremely slow if a factor has many levels (different values) since the tree has to check all possible splits into two subsets. This situation should be avoided.

Convert type into a set of 0-1 dummy variables using `class2ind`. See also `? dummyVars` in package `caret`.

```{r}
bird_train_dummy <- as_tibble(class2ind(bird_train$species)) |> 
  mutate(across(everything(), as.factor)) |>
  add_column(hab_park = bird_train$hab_park)
bird_train_dummy
```

```{r}
tree_hab_park <- bird_train_dummy |>
  rpart(hab_park~.,
        data=_,
        control= rpart.control(minsplit = 2, cp =0.01))
rpart.plot(tree_hab_park, roundint = FALSE)
```

```{r}
fit <- bird_train |> 
  train(hab_park ~ species, 
        data = _, 
        method = "rpart",
        control = rpart.control(minsplit = 2),
        tuneGrid = data.frame(cp = 0.01))
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

So species is not a good predictor of hab_park, as it seems to be a flip of the coin.

## **Class Imbalance**

Classifiers have a hard time to learn from data where we have much more observations for one class (called the majority class). This is called the class imbalance problem.

Here is a very good [article about the problem and solutions.](http://www.kdnuggets.com/2016/08/learning-from-imbalanced-classes.html)

Class distribution

```{r}
ggplot(bird_df, aes(y=species))+geom_bar()
```

To create an imbalanced problem, we want to decide if a bird is an tuftit. First, we change the class variable to make it into a binary tuftit/no tuftit classification problem. **Note:** We use here the training data for testing. You should use a separate testing data set!

```{r}
# create a new dataset
bird_tuftit <- bird_df |>
  mutate(species = factor(bird_df$species == "tuftit",
                       levels = c(FALSE, TRUE),
                       labels = c("nontuftit", "tuftit")))

```

```{r}
summary(bird_tuftit)
```

See if we have a class imbalance problem

```{r}
ggplot(bird_tuftit, aes(y=species))+geom_bar()
```

Create test and training data. I use here a 50/50 split to make sure that the test set has some samples of the rare tuftit class.

```{r}
set.seed(1234)

inTrain <- createDataPartition(y = bird_tuftit$species, p = .5, list = FALSE)
training_tuftit <- bird_tuftit |> slice(inTrain)
testing_tuftit <- bird_tuftit |> slice(-inTrain)
```

### **Option 1: Use the Data As Is and Hope For The Best**

```{r}
fit <- training_tuftit |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_tuftit),
                ref = testing_tuftit$species, positive = "tuftit")
```

Accuracy is high, but it is exactly the same as the no-information rate and kappa is zero. Sensitivity is also zero, meaning that we do not identify any positive (tuftit). If the cost of missing a positive is much larger than the cost associated with misclassifying a negative, then accuracy is not a good measure! By dealing with imbalance, we are **not** concerned with accuracy, but we want to increase the sensitivity, i.e., the chance to identify positive examples.

**Note:** The positive class value (the one that you want to detect) is set manually to tuftit using `positive = "tuftit"`. Otherwise sensitivity/specificity will not be correctly calculated.

### **Option 2: Balance Data With Resampling**

We use stratified sampling with replacement (to oversample the minority/positive class). You could also use SMOTE (in package **DMwR**) or other sampling strategies (e.g., from package **unbalanced**). We use 50+50 observations here (**Note:** many samples will be chosen several times).

```{r}
library(sampling)
set.seed(1000) # for repeatability

id <- strata(training_tuftit, stratanames = "species", size = c(50, 50), method = "srswr")
training_tuftit_balanced <- training_tuftit |> 
  slice(id$ID_unit)
table(training_tuftit_balanced$species)
```

```{r}
fit <- training_tuftit_balanced |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

Check on the unbalanced testing data.

```{r}
confusionMatrix(data = predict(fit, testing_tuftit),
                ref = testing_tuftit$species, positive = "tuftit")
```

**Note** that the accuracy is below the no information rate! However, kappa (improvement of accuracy over randomness) and sensitivity (the ability to identify tuftit) have increased.

There is a tradeoff between sensitivity and specificity (how many of the identified birds are really tuftits) The tradeoff can be controlled using the sample proportions. We can sample more tuftits to increase sensitivity at the cost of lower specificity (this effect cannot be seen in the data since the test set has only a few tuftits).

```{r}
id <- strata(training_tuftit, stratanames = "species", size = c(50, 100), method = "srswr")
training_tuftit_balanced <- training_tuftit |> 
  slice(id$ID_unit)
table(training_tuftit_balanced$species)
```

```{r}
fit <- training_tuftit_balanced |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        trControl = trainControl(method = "cv"),
        control = rpart.control(minsplit = 5))

confusionMatrix(data = predict(fit, testing_tuftit),
                ref = testing_tuftit$species, positive = "tuftit")
```

### **Option 3: Build A Larger Tree and use Predicted Probabilities**

Increase complexity and require less data for splitting a node. Here I also use AUC (area under the ROC) as the tuning metric. You need to specify the two class summary function. Note that the tree still trying to improve accuracy on the data and not AUC! I also enable class probabilities since I want to predict probabilities later.

```{r}
fit <- training_tuftit |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        tuneLength = 10,
        trControl = trainControl(method = "cv",
        classProbs = TRUE,  ## necessary for predict with type="prob"
        summaryFunction=twoClassSummary),  ## necessary for ROC
        metric = "ROC",
        control = rpart.control(minsplit = 3))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_tuftit),
                ref = testing_tuftit$species, positive = "tuftit")
```

**Note:** Accuracy is high, but it is close or below to the no-information rate!

#### Create A Biased Classifier

We can create a classifier which will detect more tuftits at the expense of misclassifying non-tuftit. This is equivalent to increasing the cost of misclassifying a tuftit as a non-tuftit. The usual rule is to predict in each node the majority class from the test data in the node. For a binary classification problem that means a probability of \>50%. In the following, we reduce this threshold to 1% or more. This means that if the new observation ends up in a leaf node with 1% or more tuftits from training then the observation will be classified as a tuftit. The data set is small and this works better with more data.

```{r}
prob <- predict(fit, testing_tuftit, type = "prob")
tail(prob)
```

```{r}
pred <- as.factor(ifelse(prob[,"tuftit"]>=0.01, "tuftit", "nontuftit"))

confusionMatrix(data = pred,
                ref = testing_tuftit$species, positive = "tuftit")
```

**Note** that accuracy goes down and is below the no information rate. However, both measures are based on the idea that all errors have the same cost. What is important is that we are now able to find more tuftits.

#### Plot the ROC Curve

Since we have a binary classification problem and a classifier that predicts a probability for an observation to be a tuftit, we can also use a [receiver operating characteristic (ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. For the ROC curve all different cutoff thresholds for the probability are used and then connected with a line. The area under the curve represents a single number for how well the classifier works (the closer to one, the better).

```{r}
library("pROC")
r <- roc(testing_tuftit$species == "tuftit", prob[,"tuftit"])
```

```{r}
r
```

```{r}
ggroc(r) + geom_abline(intercept = 1, slope = 1, color = "darkgrey")
```

### **Option 4: Use a Cost-Sensitive Classifier**

The implementation of CART in `rpart` can use a cost matrix for making splitting decisions (as parameter `loss`). The matrix has the form

TP FP FN TN

TP and TN have to be 0. We make FN very expensive (100).

```{r}
cost <- matrix(c(
  0,   1,
  100, 0
), byrow = TRUE, nrow = 2)
cost
```

```{r}
fit <- training_tuftit |> 
  train(species ~ .,
        data = _,
        method = "rpart",
        parms = list(loss = cost),
        trControl = trainControl(method = "cv"))
```

```{r}
fit
```

```{r}
rpart.plot(fit$finalModel, extra = 2)
```

```{r}
confusionMatrix(data = predict(fit, testing_tuftit),
                ref = testing_tuftit$species, positive = "tuftit")
```

The high cost for false negatives results in a classifier that does not miss any tuftit.

Using the basic concepts and techniques on the bird data shows that the 16 true/false variables about environmental factors in connection with observations of bird species do not serve as good classifiers for identification purposes. Most models had low accuracy and p-values that were not significant. These results were the same when working with class imbalances.

```{r}
# write the bird_df to data folder so that we can use it for part 2
write.csv(bird_df, "data/bird.csv", row.names = FALSE)
```

Can we predict the presence or absence of a specific bird species based on habitat type, surrounding environment (like presence of squirrels, cats, humans, etc.), and feeding habits in a given location?

I would that the answer is no. However, I limited the data I was considering and the species. I may have gotten different results if I had chosen other bird species or logical(true /false) variable. But for these 5 species and these 16 variable about the habitat and surrounding area the answer is no.
